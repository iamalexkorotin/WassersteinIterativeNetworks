{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import gc\n",
    "\n",
    "from src import distributions\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from src.resnet2 import ResNet_G, ResNet_D\n",
    "from src.unet import UNet\n",
    "\n",
    "from src.losses import InjectiveVGGPerceptualLoss, L2Loss\n",
    "\n",
    "from src.tools import unfreeze, freeze\n",
    "from src.tools import weights_init_D, weights_init_G\n",
    "from src.tools import h5py_to_dataset\n",
    "\n",
    "from src.plotters import plot_bar_random_images, plot_bar_images\n",
    "\n",
    "from src.tools import get_generated_stats, SumSequential\n",
    "from src.fid_score import calculate_frechet_distance\n",
    "\n",
    "from copy import deepcopy\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import wandb\n",
    "from src.tools import fig2data, fig2img # for wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "DEVICE_IDS = [0,1,2,3]\n",
    "\n",
    "# DATASET, DATASET_PATH = 'ave_celeba', '../../data/ave_celeba/'\n",
    "# DATASET, DATASET_PATH = 'celeba', '../../data/celeba_aligned/' # 202k Celeba Images resized to 64x64\n",
    "# DATASET, DATASET_PATH = 'mnist01', '../../data/'\n",
    "# DATASET, DATASET_PATH = 'fashionmnist_all', '../../data/'\n",
    "# DATASET, DATASET_PATH = 'fashionmnist017', '../../data/'\n",
    "DATASET, DATASET_PATH = 'handbag_shoes_fruit360', '../../data/' # shoes, handbags, fruit360\n",
    "\n",
    "G_ITERS, D_ITERS, T_ITERS = 50, 50, 10\n",
    "G_LR, D_LR, T_LR = 3e-4, 3e-4, 3e-4\n",
    "\n",
    "D_SCHEDULER_GAMMA = 0.5\n",
    "D_SCHEDULER_STEP = 10000\n",
    "G_SCHEDULER_GAMMA = 0.5\n",
    "G_SCHEDULER_STEP = 10000\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "PLOT_INTERVAL = 400\n",
    "FD_INTERVAL = 4000\n",
    "MAX_STEPS = 500001\n",
    "SEED = 0x000000\n",
    "\n",
    "REGRESSION = 'VGG' #'L2'\n",
    "SHIFT = True if DATASET == 'ave_celeba' else False # Preprocess the data with shifting means?\n",
    "EXP_NAME = f'{DATASET}_G{G_ITERS}_T{T_ITERS}_D{D_ITERS}_{REGRESSION}_{SHIFT}'\n",
    "OUTPUT_PATH = '../checkpoints/{}/'.format(DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    DATASET=DATASET, \n",
    "    G_ITERS=G_ITERS, D_ITERS=D_ITERS, T_ITERS=T_ITERS,\n",
    "    G_LR=G_LR, D_LR=D_LR, T_LR=T_LR,\n",
    "    BATCH_SIZE=BATCH_SIZE,\n",
    "    REGRESSION=REGRESSION,\n",
    "    D_SCHEDULER_GAMMA=D_SCHEDULER_GAMMA,\n",
    "    D_SCHEDULER_STEP=D_SCHEDULER_STEP,\n",
    "    G_SCHEDULER_GAMMA=G_SCHEDULER_GAMMA,\n",
    "    G_SCHEDULER_STEP=G_SCHEDULER_STEP,\n",
    "    SHIFT=SHIFT,\n",
    ")\n",
    "    \n",
    "assert torch.cuda.is_available()\n",
    "torch.cuda.set_device(f'cuda:{DEVICE_IDS[0]}')\n",
    "torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.makedirs(OUTPUT_PATH)\n",
    "\n",
    "if REGRESSION == 'VGG':\n",
    "    vgg_loss = InjectiveVGGPerceptualLoss().cuda()\n",
    "    if len(DEVICE_IDS) > 1:\n",
    "        vgg_loss = nn.DataParallel(vgg_loss, device_ids=DEVICE_IDS)\n",
    "    \n",
    "G_criterion = L2Loss() if REGRESSION == 'L2' else vgg_loss\n",
    "\n",
    "if DATASET == 'ave_celeba':\n",
    "    IMG_SIZE, SIZE, NC, Z_DIM = 64, 200000, 3, 128\n",
    "    ALPHAS = [0.25, 0.5, 0.25]\n",
    "    CLASSES = [0, 1, 2]\n",
    "elif DATASET == 'celeba':\n",
    "    IMG_SIZE, SIZE, NC, Z_DIM = 64, 200000, 3, 128\n",
    "    ALPHAS = [1.]\n",
    "    CLASSES = [0]\n",
    "elif DATASET == 'fashionmnist_all':\n",
    "    IMG_SIZE, SIZE, NC, Z_DIM = 32, 6000, 1, 16\n",
    "    ALPHAS = [0.1 for _ in range(10)]\n",
    "    CLASSES = list(range(10))\n",
    "elif DATASET == 'fashionmnist017':\n",
    "    IMG_SIZE, SIZE, NC, Z_DIM = 32, 6000, 1, 16\n",
    "    ALPHAS = [1/3. for _ in range(3)]\n",
    "    CLASSES = [0,1,7]\n",
    "elif DATASET == 'mnist01':\n",
    "    IMG_SIZE, SIZE, NC, Z_DIM = 32, 6000, 1, 16\n",
    "    ALPHAS = [0.5, .5]\n",
    "    CLASSES = [0,1]\n",
    "elif DATASET == 'mnist01':\n",
    "    IMG_SIZE, SIZE, NC, Z_DIM = 32, 6000, 1, 16\n",
    "    ALPHAS = [0.5, .5]\n",
    "    CLASSES = [0,1]\n",
    "elif DATASET == 'handbag_shoes_fruit360':\n",
    "    IMG_SIZE, SIZE, NC, Z_DIM = 64, 6000, 3, 128\n",
    "    ALPHAS = [1./3, 1./3, 1./3]\n",
    "    CLASSES = None\n",
    "else:\n",
    "    raise Exception('Unknown dataset')\n",
    "    \n",
    "K = len(ALPHAS)\n",
    "INCEPTION = True if 'celeba' in DATASET else False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Samplers (Z, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_sampler = distributions.StandardNormalSampler(dim=Z_DIM)\n",
    "Y_samplers = []\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Pad(14, fill=(255,255,255)) if DATASET == 'handbag_shoes_fruit360' else torchvision.transforms.Identity(),\n",
    "    torchvision.transforms.Resize(IMG_SIZE),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Lambda(lambda x: 2 * x - 1)\n",
    "])\n",
    "\n",
    "if DATASET != 'handbag_shoes_fruit360': \n",
    "    for k in range(K):\n",
    "        if DATASET in ['ave_celeba', 'celeba']:\n",
    "            dataset = torchvision.datasets.ImageFolder(DATASET_PATH, transform=transform)\n",
    "        elif 'fashionmnist' in DATASET:\n",
    "            dataset = torchvision.datasets.FashionMNIST(root=DATASET_PATH, download=True, transform=transform, train=True)\n",
    "        elif 'mnist' in DATASET:\n",
    "            dataset = torchvision.datasets.MNIST(root=DATASET_PATH, download=True, transform=transform)\n",
    "        else:\n",
    "            raise Exception('Unknown dataset')\n",
    "\n",
    "        try:\n",
    "            dataset.samples = [s for s in dataset.samples if s[1] == CLASSES[k]]\n",
    "        except:\n",
    "            idx = [t == CLASSES[k] for t in dataset.targets]\n",
    "            if 'mnist' in DATASET:\n",
    "                dataset.targets, dataset.data = np.array(dataset.targets)[idx], torch.tensor(dataset.data)[idx]\n",
    "            else:\n",
    "                dataset.targets, dataset.data = np.array(dataset.targets)[idx], np.array(dataset.data)[idx]\n",
    "\n",
    "        Y_samplers.append(distributions.DatasetSampler(dataset))\n",
    "        \n",
    "elif DATASET == 'handbag_shoes_fruit360':\n",
    "    dataset = h5py_to_dataset(os.path.join(DATASET_PATH, 'handbag_64.hdf5'))\n",
    "    Y_samplers.append(distributions.DatasetSampler(dataset))\n",
    "    dataset = h5py_to_dataset(os.path.join(DATASET_PATH, 'shoes_64.hdf5'))\n",
    "    Y_samplers.append(distributions.DatasetSampler(dataset))\n",
    "    dataset = torchvision.datasets.ImageFolder(os.path.join(DATASET_PATH, 'fruit360'), transform=transform)\n",
    "    Y_samplers.append(distributions.DatasetSampler(dataset))\n",
    "    \n",
    "if SHIFT:\n",
    "    with torch.no_grad():\n",
    "        Y_bar_mean = 0.\n",
    "        Y_means = []\n",
    "        for k in range(K):\n",
    "            Y_means.append(Y_samplers[k].dataset.mean(dim=0))\n",
    "            Y_bar_mean += ALPHAS[k] * Y_means[-1]\n",
    "        for k in range(K):\n",
    "            Y_samplers[k].dataset += Y_bar_mean - Y_means[k]\n",
    "    \n",
    "torch.cuda.empty_cache(); gc.collect()\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_name = None\n",
    "if DATASET in ['ave_celeba', 'celeba']:\n",
    "    stats_name = 'celeba'\n",
    "elif DATASET == 'fashionmnist_all':\n",
    "    stats_name = 'fashionmnist_bar'\n",
    "elif DATASET == 'fashionmnist017':\n",
    "    stats_name = 'fashionmnist017'\n",
    "elif DATASET == 'mnist01':\n",
    "    stats_name = 'mnist01'\n",
    "elif DATASET == 'handbag_shoes_fruit360':\n",
    "    pass\n",
    "else:\n",
    "    raise Exception('Unknown dataset')\n",
    "\n",
    "if stats_name is not None:\n",
    "    with open('../stats/{}.json'.format(stats_name), 'r') as f:\n",
    "        stats = json.load(f)\n",
    "        mu_data, sigma_data = stats['mu'], stats['sigma']\n",
    "        del stats\n",
    "        gc.collect()\n",
    "else:\n",
    "    mu_data, sigma_data = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = ResNet_G(Z_DIM, IMG_SIZE, nc=NC).cuda()\n",
    "G.apply(weights_init_D)\n",
    "\n",
    "Ds = []\n",
    "for k in range(K):\n",
    "    Ds.append(ResNet_D(IMG_SIZE, nc=NC).cuda())\n",
    "    Ds[-1].apply(weights_init_D)\n",
    "\n",
    "Ts = []\n",
    "for k in range(K):\n",
    "    Ts.append(UNet(NC, NC, base_factor=48 if NC == 3 else 16).cuda())\n",
    "\n",
    "if len(DEVICE_IDS) > 1:\n",
    "    G = nn.DataParallel(G, device_ids=DEVICE_IDS)\n",
    "    for k in range(K):\n",
    "        Ts[k] = nn.DataParallel(Ts[k], device_ids=DEVICE_IDS)\n",
    "        Ds[k] = nn.DataParallel(Ds[k], device_ids=DEVICE_IDS)\n",
    "    \n",
    "print('G params:', np.sum([np.prod(p.shape) for p in G.parameters()]))\n",
    "print('T params:', np.sum([np.prod(p.shape) for p in Ts[0].parameters()]))\n",
    "print('D params:', np.sum([np.prod(p.shape) for p in Ds[0].parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix random images\n",
    "Z_fixed = Z_sampler.sample(10)\n",
    "Ys_fixed = [Y_samplers[k].sample(10) for k in range(K)]\n",
    "Y_pack_fixed = [Y_samplers[k].sample(1)[0] for k in range(K)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plot_bar_images(Z_fixed, Ys_fixed, G, Ts, ALPHAS)\n",
    "fig, axes = plot_bar_random_images(Z_sampler, Y_samplers, G, Ts, ALPHAS)\n",
    "if hasattr(Ts[0], 'inverse'):\n",
    "    fig, axes = plot_bar_maps(Y_pack_fixed, Ts)\n",
    "    fig, axes = plot_random_bar_maps(Y_samplers, Ts)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(name=EXP_NAME, project='wasserstein2iterativenetworks', entity='gunsandroses', config=config)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ts_opt, Ds_opt = [], []\n",
    "\n",
    "G_opt = torch.optim.Adam(G.parameters(), lr=G_LR, weight_decay=1e-7)\n",
    "for k in range(K):\n",
    "    Ts_opt.append(torch.optim.Adam(Ts[k].parameters(), lr=T_LR, weight_decay=1e-10))\n",
    "    Ds_opt.append(torch.optim.Adam(Ds[k].parameters(), lr=D_LR, weight_decay=1e-10))\n",
    "\n",
    "G_sch = torch.optim.lr_scheduler.StepLR(G_opt, step_size=G_SCHEDULER_STEP, gamma=G_SCHEDULER_GAMMA)\n",
    "Ds_sch = []   \n",
    "for k in range(K):\n",
    "    Ds_sch.append(torch.optim.lr_scheduler.StepLR(Ds_opt[k], step_size=D_SCHEDULER_STEP, gamma=D_SCHEDULER_GAMMA))\n",
    "\n",
    "last_plot_step, last_fd_step = -np.inf, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "while step < MAX_STEPS:\n",
    "    freeze(G)\n",
    "    \n",
    "    for k in range(K):\n",
    "        # D and T optimization cycle\n",
    "        for d_iter in tqdm(range(D_ITERS)):\n",
    "            step += 1\n",
    "\n",
    "            # T optimization\n",
    "            unfreeze(Ts[k]); freeze(Ds[k])\n",
    "            for t_iter in range(T_ITERS): \n",
    "                with torch.no_grad():\n",
    "                    X = G(Z_sampler.sample(BATCH_SIZE))\n",
    "                Ts_opt[k].zero_grad()\n",
    "                T_X = Ts[k](X)\n",
    "                T_loss = .5 * F.mse_loss(X, T_X) - Ds[k](T_X).mean()\n",
    "                T_loss.backward(); Ts_opt[k].step()\n",
    "            del T_loss, T_X, X; gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "            # D optimization\n",
    "            with torch.no_grad():\n",
    "                X = G(Z_sampler.sample(BATCH_SIZE))\n",
    "            Y = Y_samplers[k].sample(BATCH_SIZE)\n",
    "            unfreeze(Ds[k]); freeze(Ts[k])\n",
    "            with torch.no_grad():\n",
    "                T_X = Ts[k](X)  \n",
    "            Ds_opt[k].zero_grad()\n",
    "            D_loss = Ds[k](T_X).mean() - Ds[k](Y).mean()\n",
    "            D_loss.backward(); Ds_opt[k].step(); Ds_sch[k].step()\n",
    "            wandb.log({f'D_loss_{k}' : D_loss.item()}, step=step) \n",
    "            del D_loss, Y, X; gc.collect(); torch.cuda.empty_cache()\n",
    "        \n",
    "    if step >= last_plot_step + PLOT_INTERVAL:\n",
    "        print('Plotting')\n",
    "        last_plot_step = step; clear_output(wait=True)\n",
    "        \n",
    "        fig, axes = plot_bar_images(Z_fixed, Ys_fixed, G, Ts, ALPHAS)\n",
    "        wandb.log({'Fixed Images' : [wandb.Image(fig2img(fig))]}, step=step) \n",
    "        plt.show(fig); plt.close(fig) \n",
    "\n",
    "        fig, axes = plot_bar_random_images(Z_sampler, Y_samplers, G, Ts, ALPHAS)\n",
    "        wandb.log({'Random Images' : [wandb.Image(fig2img(fig))]}, step=step) \n",
    "        plt.show(fig); plt.close(fig) \n",
    "    \n",
    "    if step >= last_fd_step + FD_INTERVAL:\n",
    "        last_fd_step = step\n",
    "        \n",
    "        if mu_data is not None:\n",
    "            print('Computing FD score')\n",
    "            m, s = get_generated_stats(G, Z_sampler, size=SIZE, batch_size=8, inception=INCEPTION, verbose=True)\n",
    "            FD_G = calculate_frechet_distance(m, s, mu_data, sigma_data)\n",
    "            del m, s;  gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "            T_G = SumSequential(G, Ts, ALPHAS)\n",
    "            m, s = get_generated_stats(T_G, Z_sampler, size=SIZE, inception=INCEPTION, batch_size=8, verbose=True)\n",
    "            FD_T_G = calculate_frechet_distance(m, s, mu_data, sigma_data)\n",
    "            del m, s;  gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "            wandb.log({'FD_G' : FD_G, 'FD_T_G' : FD_T_G}, step=step)\n",
    "        \n",
    "        print('Creating a checkpoint')\n",
    "        freeze(G); torch.save(G.state_dict(), os.path.join(OUTPUT_PATH, f'G_{step}.pt'))\n",
    "        for k in range(K):\n",
    "            freeze(Ts[k]); \n",
    "            torch.save(Ts[k].state_dict(), os.path.join(OUTPUT_PATH, f'T_{k}_{step}.pt'))\n",
    "    \n",
    "    # G optimization\n",
    "    if G_ITERS > 0:\n",
    "        for k in range(K):\n",
    "            freeze(Ts[k])\n",
    "        G_old = deepcopy(G); freeze(G_old)\n",
    "        unfreeze(G)\n",
    "        for g_iter in range(G_ITERS):\n",
    "            step += 1\n",
    "            Z = Z_sampler.sample(BATCH_SIZE)\n",
    "            with torch.no_grad():\n",
    "                G_old_Z = G_old(Z)\n",
    "                T_G_old_Z = torch.zeros_like(G_old(Z))\n",
    "                for k in range(K):\n",
    "                    T_G_old_Z += ALPHAS[k] * Ts[k](G_old(Z))\n",
    "\n",
    "            G_opt.zero_grad()\n",
    "            G_loss = .5 * G_criterion(G(Z), T_G_old_Z).mean()\n",
    "            G_loss.backward(); G_opt.step(); G_sch.step()\n",
    "\n",
    "            wandb.log({\"G_loss\" : G_loss.item()}, step=step)\n",
    "\n",
    "        del G_old, G_loss, T_G_old_Z, Z\n",
    "        gc.collect(); torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
